# Todo:

(Update it with the progres you've made, add whatever you need to)

## Web Crawling

- [ ] Add checks for html pages only
- [ ] Optimize
- [x] Dockerize scraping for kubernetes deployment
    - [ ] Add checks for stalled scraping, reporting to the dashboard
- [ ] Come up with more chracteristics about scraped domains to store and use in search algorithm
- [x] Make rock-solid domain wait times, don't hit a domain accross all scrapers in less than 5 seconds
- [ ] **Respect 400/500 error responses and stop crawling the domain**
- [ ] Look for and respect Crawl-Delay in robots.txt
- [ ] Should report http reponses to local lookup table and therefore the index database
- [ ] Sending data to the database should spawn a new process instead of waiting for the database transaction to finish
- [ ] Wikipedia is under strain due to AI crawlers, manually download the wikipedia from Wikimedia dumps and scrape the local dump instead of scraping
- [ ] Handle scraping pdf files
- [ ] Handle urls which redirect to another (probably just resolve all urls) (for example https://woo.app)
- [ ] Still tries to scrape image files, fix that
- [ ] Add TFIDF to index data

## Database

- [ ] Switch to a postgres cluster
- [ ] Research distributed sql instead of postgres clusters
- [ ] Optimize searching
- [ ] Enter urls into queue with delay for scraping
- [x] Set up Redis DB for domain scraping cooldown

## Hardware

- [x] Figure out the hardware

## Dashboard

- [ ] Add number of urls in the queue

## Misc

- [x] Write the dashboard for monitoring scraping progress. How many sites/pages have been crawled, how much is stored in the database, how many links have been foloowed, etc. Make a public version?
- [ ] Look into proxying traffic through cloud services
- [ ] Make a config system for new scrapers to pull from, possily integrate with k8?
- [ ] Make a web page or about page + email address on Github to for abuse complaints
- [x] Come up wth a name
- [ ] Need to add language detection somewhere, only scrape English? Or we could translate pages on the hpc and serve up the translated versions
- [ ] Need to save the raw text of pages so we can do analysis and research on it later

## Cool Things to Add

- [ ] Build an ml model or something else to determine whether a page was generated by AI to exclude or flag that result in the search
- [ ] Add the ability for users to choose the weights for bigrams/trigrams/prefixes/etc, choose how the algorithm works for themselves
- [ ] Add Listicles like Kagi does
- [ ] Add blocklists (disinformation domains, marketing domains, gambling domains, malware domains, etc) for users to set to block or a warning when a result is in the search result
- [ ] Add support for regex in search
- [ ] Add duckuckgo bangs [https://github.com/kagisearch/bangs](https://github.com/kagisearch/bangs)
- [ ] Add an option to only include domains which have appeared in hacker news (https://www.kaggle.com/datasets/hacker-news/hacker-news)
- [ ] Add an option to only search Kagi Smallweb
- [ ] Add categorizing of domains in search and/or in the scraping progress dashboard [https://domainsproject.org/dataset](https://domainsproject.org/dataset)
- [ ] Host the search over DNS. Something very similar was done with Wikipedia summaries, [https://dgl.cx/2008/10/wikipedia-summary-dns](https://dgl.cx/2008/10/wikipedia-summary-dns) you can have a domain name be the search query and have a super optimized dns response with the results
- [ ] Could classify text for each page with sentiment analysis
- [ ] Add onion site
- [ ] Scrape only academic sites (and save into a DB so we can create a scholastic LLM) (scholar.google.com)
- [ ] Use ML on the internet dataset from Stultus to measure how much of the web is positive vs negative
- [ ] Classify internet dataset into categories, (eg news categories, industry topics, or content themes), can be used in searching
- [ ] Use Named Entity Recognition (NER) to extract specific entity types like people, organizations, locations, and dates from the internet dataset to analyze where on the internet they appear, including through page categories (see above)